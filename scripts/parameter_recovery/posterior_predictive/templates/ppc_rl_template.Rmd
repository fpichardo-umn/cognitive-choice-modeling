
## RL Component Analysis

This section examines core reinforcement learning behavioral metrics across participants.

### Metrics Summary

```{r rl_component_metrics, echo=FALSE, fig.width=10, fig.height=8}
# Filter for RL component data
rl_subject_data <- subject_data %>% filter(component == "RL")

# Key RL statistics to analyze
key_rl_stats <- c("rl_avg_outcome", "rl_good_deck_ratio", "rl_lose_shift_ratio", 
                 "rl_new_bad_deck_ratio", "rl_new_good_deck_ratio", 
                 "rl_skip_ratio", "rl_total_money", "rl_win_stay_ratio")

# Filter for key statistics
rl_key_data <- rl_subject_data %>%
  filter(statistic %in% key_rl_stats)

if(nrow(rl_key_data) > 0) {
  # Calculate misfit statistics
  rl_key_data <- rl_key_data %>%
    mutate(
      misfit = observed - predicted_mean,
      misfit_standardized = misfit / (predicted_q975 - predicted_q025) * 2,
      misfit_direction = case_when(
        misfit > 0 & ppp_extreme ~ "Model Underpredicts",
        misfit < 0 & ppp_extreme ~ "Model Overpredicts",
        TRUE ~ "Good Fit"
      )
    )
  
  # Plot 1: Observed vs Predicted across subjects by statistic
  p1 = ggplot(rl_key_data, aes(x = predicted_mean, y = observed)) +
    geom_point(aes(color = misfit_direction), size = 3, alpha = 0.7) +
    geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
    facet_wrap(~statistic, scales = "free") +
    scale_color_manual(values = c("Good Fit" = "gray", 
                                 "Model Underpredicts" = "blue", 
                                 "Model Overpredicts" = "red")) +
    theme_minimal() +
    labs(title = "RL Metrics: Observed vs. Predicted Values",
         subtitle = "Points on the diagonal indicate perfect prediction",
         x = "Predicted Value", y = "Observed Value",
         color = "Misfit Direction")
  
  # Plot 2: Misfit magnitude and direction
  p2 = ggplot(rl_key_data, aes(x = reorder(statistic, abs(misfit_standardized), FUN = median), 
                             y = misfit_standardized, fill = misfit_direction)) +
  # Use boxplot without outliers
  geom_boxplot(outlier.shape = NA) + 
  # Add points to show all data including outliers
  geom_point(position = position_jitter(width = 0.2, height = 0), 
             alpha = 0.7, size = 2) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_hline(yintercept = c(-1, 1), linetype = "dotted", color = "red") +
  scale_fill_manual(values = c("Good Fit" = "gray", 
                              "Model Underpredicts" = "blue", 
                              "Model Overpredicts" = "red")) +
  # Limit the main view but don't clip points
  coord_flip(ylim = c(-3, 3), clip = "off") + 
  # Add a secondary axis with a larger range for context
  scale_y_continuous(
    sec.axis = sec_axis(~., breaks = c(-100, -50, -10, 0, 10, 50, 100), 
                        name = "Extended Scale (for outliers)")
  ) +
  theme_minimal() +
  # Improve theme elements for better readability
  theme(
    panel.grid.minor = element_blank(),
    plot.margin = margin(r = 40, l = 10, t = 10, b = 10)  # Add extra margin on right side
  ) +
  labs(title = "RL Metrics: Direction and Magnitude of Misfit",
       subtitle = "Main view limited to -3 to 3 range, outliers still visible",
       x = "", y = "Standardized Misfit",
       fill = "Direction")
  
  # Create summary table
  rl_summary <- rl_key_data %>%
    group_by(statistic) %>%
    summarize(
      mean_observed = mean(observed, na.rm = TRUE),
      sd_observed = sd(observed, na.rm = TRUE),
      mean_predicted = mean(predicted_mean, na.rm = TRUE),
      mean_misfit = mean(misfit, na.rm = TRUE),
      extreme_rate = mean(ppp_extreme, na.rm = TRUE),
      underpredict_rate = mean(misfit_direction == "Model Underpredicts", na.rm = TRUE),
      overpredict_rate = mean(misfit_direction == "Model Overpredicts", na.rm = TRUE),
      .groups = "drop"
    ) %>%
    arrange(desc(extreme_rate))
  
  knitr::knit_print(kable(rl_summary, caption = "Summary of RL Behavioral Metrics", digits = 3) %>%
    kable_styling(bootstrap_options = c("striped", "hover")))
  knitr::knit_print(p1)
  knitr::knit_print(p2)
}
```


### Win-Stay/Lose-Shift Behavior

```{r rl_wsls, echo=FALSE, fig.width=8, fig.height=5}
# Extract WSLS data
wsls_data <- subject_data %>%
  filter(component == "RL" & statistic %in% c("rl_win_stay_ratio", "rl_lose_shift_ratio"))

if(nrow(wsls_data) > 0) {
  # Aggregate across subjects
  wsls_agg <- wsls_data %>%
    group_by(statistic) %>%
    summarize(
      obs_mean = mean(observed, na.rm = TRUE),
      obs_se = sd(observed, na.rm = TRUE)/sqrt(n()),
      pred_mean = mean(predicted_mean, na.rm = TRUE),
      pred_lower = mean(predicted_q025, na.rm = TRUE),
      pred_upper = mean(predicted_q975, na.rm = TRUE),
      .groups = "drop"
    )
  
  # Plot aggregated WSLS data
  p1 = ggplot(wsls_agg, aes(x = statistic)) +
    geom_pointrange(aes(y = pred_mean, ymin = pred_lower, ymax = pred_upper), 
                   color = "blue", size = 1, position = position_dodge(width = 0.5)) +
    geom_pointrange(aes(y = obs_mean, ymin = obs_mean - obs_se, ymax = obs_mean + obs_se), 
                   color = "red", shape = 4, size = 1, position = position_dodge(width = 0.5)) +
    ylim(0, 1) +
    theme_minimal() +
    labs(title = "Win-Stay/Lose-Shift Behavior", 
         subtitle = "Red X = Observed (with SE), Blue dots = Predicted (with 95% CI)",
         x = "", y = "Ratio")
         
  # Show misfit
  wsls_agg <- wsls_agg %>% 
    mutate(misfit = obs_mean - pred_mean,
           misfit_standardized = misfit / (pred_upper - pred_lower) * 2)
           
  knitr::knit_print(kable(wsls_agg %>% select(statistic, obs_mean, pred_mean, misfit, misfit_standardized),
        caption = "Win-Stay/Lose-Shift: Observed vs. Predicted") %>%
    kable_styling(bootstrap_options = c("striped", "hover")))
  knitr::knit_print(p1)
}
```


### Deck Selection Stats

```{r deck_selection_ratio}
deck_ratio_data <- subject_data %>%
  filter(component == "RL" & 
         statistic %in% c("rl_good_deck_ratio", "rl_new_good_deck_ratio", "rl_new_bad_deck_ratio", "rl_skip_ratio"))

if(nrow(deck_ratio_data) > 0) {
  # Aggregate across subjects
  ratio_agg <- deck_ratio_data %>%
    group_by(statistic) %>%
    summarize(
      obs_mean = mean(observed, na.rm = TRUE),
      obs_se = sd(observed, na.rm = TRUE)/sqrt(n()),
      pred_mean = mean(predicted_mean, na.rm = TRUE),
      pred_lower = mean(predicted_q025, na.rm = TRUE),
      pred_upper = mean(predicted_q975, na.rm = TRUE),
      misfit = obs_mean - pred_mean,
      .groups = "drop"
    )
  
  # Plot deck selection ratios
  p1 = ggplot(ratio_agg, aes(x = statistic)) +
    geom_pointrange(aes(y = pred_mean, ymin = pred_lower, ymax = pred_upper), 
                   color = "blue", size = 1, position = position_dodge(width = 0.5)) +
    geom_pointrange(aes(y = obs_mean, ymin = obs_mean - obs_se, ymax = obs_mean + obs_se), 
                   color = "red", shape = 4, size = 1, position = position_dodge(width = 0.5)) +
    ylim(0, 1) +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
    labs(title = "Deck Selection Performance", 
         subtitle = "Red X = Observed (with SE), Blue dots = Predicted (with 95% CI)",
         x = "", y = "Ratio")
         
  # Plot directionality of misfit
  p2 = ggplot(ratio_agg, aes(x = statistic, y = misfit, fill = abs(misfit) > 0.05)) +
    geom_col() +
    geom_hline(yintercept = 0, linetype = "dashed") +
    scale_fill_manual(values = c("TRUE" = "red", "FALSE" = "gray"), 
                     labels = c("Not Significant", "Significant"),
                     name = "Misfit") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
    labs(title = "Misfit Direction for Deck Selection Metrics",
         subtitle = "Positive = Model Underpredicts, Negative = Model Overpredicts",
         x = "", y = "Observed - Predicted")
  knitr::knit_print(p1)
  knitr::knit_print(p2)
}
```

### Performance Stats

```{r}
# Extract monetary data
monetary_data <- subject_data %>%
  filter(component == "RL" & statistic %in% c("rl_total_money", "rl_avg_outcome"))
if(nrow(monetary_data) > 0) {
  # Aggregate across subjects
  money_agg <- monetary_data %>%
    group_by(statistic) %>%
    summarize(
      obs_mean = mean(observed, na.rm = TRUE),
      obs_se = sd(observed, na.rm = TRUE)/sqrt(n()),
      pred_mean = mean(predicted_mean, na.rm = TRUE),
      pred_lower = mean(predicted_q025, na.rm = TRUE),
      pred_upper = mean(predicted_q975, na.rm = TRUE),
      misfit_pct = (obs_mean - pred_mean) / pred_mean * 100,
      .groups = "drop"
    )
  
  # Calculate y-limit ranges for each facet to place text properly
  money_agg <- money_agg %>%
    group_by(statistic) %>%
    mutate(
      y_max = max(c(pred_upper, obs_mean + obs_se), na.rm = TRUE),
      text_y = y_max * 1.15
    )
  
  # Plot monetary metrics with percentage difference
  p1 = ggplot(money_agg, aes(x = statistic)) +
    # Predicted values (with error bars)
    geom_pointrange(aes(y = pred_mean, ymin = pred_lower, ymax = pred_upper), 
                   color = "blue", size = 1) +
    # Observed values (with SE as error bars)
    geom_pointrange(aes(y = obs_mean, ymin = obs_mean - obs_se, ymax = obs_mean + obs_se), 
                   color = "red", shape = 4, size = 1) +
    # Text label for percentage difference
    geom_text(aes(y = text_y, label = sprintf("Î”: %.1f%%", misfit_pct)),
              size = 4,
              fontface = "bold") +
    # Essential: Use facet_wrap with free_y scales
    facet_wrap(~statistic, scales = "free_y", labeller = labeller(statistic = 
                                                                 c("rl_total_money" = "Total Money",
                                                                   "rl_avg_outcome" = "Average Outcome"))) +
    # Improve x-axis presentation
    scale_x_discrete(labels = NULL) +  # Remove x-axis labels since they're redundant with facet titles
    theme_minimal() +
    theme(
      strip.text = element_text(size = 14, face = "bold"),  # Larger facet labels
      axis.text.y = element_text(size = 10),  # Larger y axis text
      axis.ticks.x = element_blank(),  # Remove x-axis ticks
      axis.text.x = element_blank(),   # Remove x-axis labels
      plot.title = element_text(size = 14, face = "bold"), 
      plot.subtitle = element_text(size = 12),
      panel.grid.major.x = element_blank(),  # Remove vertical grid lines
      panel.grid.minor = element_blank()  # Remove minor grid lines
    ) +
    labs(
      title = "Monetary Performance Metrics", 
      subtitle = "Red X = Observed (with SE), Blue dots = Predicted (with 95% CI)",
      x = "", y = "Value"
    )
    
  # Table of monetary metrics
  knitr::knit_print(kable(money_agg, caption = "Monetary Metrics Summary") %>%
    kable_styling(bootstrap_options = c("striped", "hover")))
  
  # Display plot
  knitr::knit_print(p1)
}
```


## Parameter/Data Correlations
```{r rl_param_correlation_analysis, fig.width=10, fig.height=8, results='asis'}
# Only proceed if recovery data is available
if (!is.null(recovery_data)) {
  # Extract RL parameters from recovery data
  rl_params <- recovery_data %>%
    filter(parameter_type %in% c("individual", "group", "single")) %>%
    # Filter out SSM-specific parameters
    filter(!grepl("ndt|boundary|threshold|drift|tau|beta", parameter, ignore.case = TRUE)) %>%
    select(subject_id, parameter, true_value, recovered_value) %>%
    # We'll use recovered values for correlations (since these are what the model "thinks" it has)
    select(subject_id, parameter, recovered_value) %>%
    pivot_wider(names_from = parameter, values_from = recovered_value)
  
  # Extract RL metrics from PPC data
  rl_ppc_stats <- subject_data %>%
    filter(component == "RL") %>%
    select(subject_id, statistic, observed) %>%
    pivot_wider(names_from = statistic, values_from = observed)
  
  # Only proceed if we have enough data
  if (ncol(rl_params) > 1 && ncol(rl_ppc_stats) > 1) {
    # Join datasets by subject_id
    rl_params$subject_id = rl_ppc_stats$subject_id
    rl_combined <- inner_join(rl_params, rl_ppc_stats, by = "subject_id")
    
    # Select only numeric columns for correlation
    rl_numeric <- rl_combined %>% select_if(is.numeric)
    
    # Separate parameters and statistics for clearer presentation
    rl_param_cols <- setdiff(names(rl_params), "subject_id")
    rl_stat_cols <- setdiff(names(rl_ppc_stats), "subject_id")
    
    # Create correlation matrix between parameters and PPC statistics
    rl_cor_matrix <- cor(rl_numeric[rl_param_cols], rl_numeric[rl_stat_cols], 
                         use = "pairwise.complete.obs")
    
    # Plot parameter-behavior correlations
    success <- create_corr_plot(rl_cor_matrix, "RL Parameters vs. Behavioral Metrics")
    
    if (success) {
      # Create table of strongest correlations
      rl_cors_long <- get_top_correlations(rl_cor_matrix, 10)
      
      # Create a nicer table of top correlations
      rl_cors_long %>%
        mutate(
          Parameter = format_names(Parameter),
          Statistic = format_names(Statistic)
        ) %>%
        kable(caption = "Top 10 Parameter-Behavior Correlations", digits = 3) %>%
        kable_styling(bootstrap_options = c("striped", "hover", "condensed"), 
                      full_width = FALSE) %>%
        knitr::knit_print()
      
      # Create parameter-parameter correlation matrix
      param_cor <- cor(rl_numeric[rl_param_cols], use = "pairwise.complete.obs")
      create_corr_plot(param_cor, "Parameter-Parameter Correlations")
      
      # Create behavior-behavior correlation matrix
      behavior_cor <- cor(rl_numeric[rl_stat_cols], use = "pairwise.complete.obs")
      create_corr_plot(behavior_cor, "Behavioral Metric Correlations")
      
      # Identify high-impact parameters (those with strongest and most correlations)
      param_impact <- rl_cors_long %>%
        group_by(Parameter) %>%
        summarize(
          Mean_Abs_Correlation = mean(Abs_Correlation, na.rm = TRUE),
          Max_Correlation = max(Correlation, na.rm = TRUE),
          Min_Correlation = min(Correlation, na.rm = TRUE),
          Impact_Score = sum(Abs_Correlation, na.rm = TRUE),
          Num_Strong_Correlations = sum(Abs_Correlation > 0.4, na.rm = TRUE),
          .groups = "drop"
        ) %>%
        arrange(desc(Impact_Score))
      
      # Show parameter impact table
      param_impact %>%
        mutate(Parameter = format_names(Parameter)) %>%
        kable(caption = "Parameter Impact Rankings (sum of absolute correlations)", digits = 3) %>%
        kable_styling(bootstrap_options = c("striped", "hover", "condensed"),
                      full_width = FALSE) %>%
        knitr::knit_print()
      
      # Visualize top parameter impacts
      if (nrow(param_impact) >= 3) {
        top_params <- head(param_impact, 5)$Parameter
        
        # Extract correlations for top parameters
        top_cors <- rl_cors_long %>%
          filter(Parameter %in% top_params) %>%
          mutate(
            Parameter = factor(Parameter, levels = rev(top_params)),
            Statistic = gsub("_", " ", Statistic)
          )
        
        # Create a horizontal bar plot
        ggplot(top_cors, aes(x = reorder(Statistic, Abs_Correlation), 
                              y = Correlation, fill = Parameter)) +
          geom_col(position = "dodge") +
          coord_flip() +
          scale_fill_brewer(palette = "Set1") +
          theme_minimal() +
          labs(title = "Top Parameters' Influence on Behavioral Metrics",
               x = "",
               y = "Correlation Strength") +
          theme(legend.title = element_blank())
        
        # Create a heatmap alternative
        if (length(top_params) >= 3 && length(unique(top_cors$Statistic)) >= 3) {
          ggplot(top_cors, aes(x = Parameter, y = Statistic, fill = Correlation)) +
            geom_tile() +
            scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
                                midpoint = 0, limit = c(-1,1)) +
            theme_minimal() +
            theme(axis.text.x = element_text(angle = 45, hjust = 1),
                  axis.text.y = element_text(size = 8)) +
            labs(title = "Heatmap of Top Parameter-Behavior Correlations")
        }
    }
   } else {
      cat("Insufficient data for correlation analysis.n")
    }
  } else {
    cat("Insufficient parameter or PPC data for correlation analysis.n")
  }
} else {
  cat("Recovery data not available for correlation analysis.n")
}
```



### Network Plot
```{r network_plot, fig.width=10, fig.height=8, results='asis'}
# Define correlation threshold as a variable
cor_threshold <- 0.1  # Minimum correlation magnitude to consider

# Get number of observations (subjects)
n_subjects <- nrow(rl_numeric)

# Calculate significance and filter
significant_cors <- rl_cors_long %>%
  mutate(
    # Calculate p-value using t-distribution
    p_value = 2 * pt(abs(Correlation) * sqrt(n_subjects - 2) / sqrt(1 - Correlation^2), 
                    df = n_subjects - 2, lower.tail = FALSE),
    # Flag for significance
    is_significant = p_value < 0.05
  ) %>%
  filter(is_significant & Abs_Correlation > cor_threshold) %>%
  arrange(desc(Abs_Correlation)) %>%
  head(30)  # Take top 30 by correlation magnitude AFTER filtering for significance

if (nrow(significant_cors) > 5) {
  # Create graph
  g <- graph_from_data_frame(significant_cors[, c("Parameter", "Statistic", "Correlation")], directed = TRUE)
  
  # Add node attributes (REVERSED as requested: parameters are circles, metrics are squares)
  V(g)$type <- ifelse(V(g)$name %in% significant_cors$Parameter, "Parameter", "Statistic")
  
  # Set edge width based on correlation strength
  E(g)$width <- 1 + 4 * abs(E(g)$Correlation)  # Scale width by correlation magnitude
  
  # Set edge color based on positive/negative correlation
  E(g)$color <- ifelse(E(g)$Correlation > 0, "#4DAF4A", "#E41A1C")
  
  # Set node shape based on type (REVERSED as requested)
  V(g)$shape <- ifelse(V(g)$type == "Parameter", "circle", "square")
  
  # Set node color based on type
  V(g)$color <- ifelse(V(g)$type == "Parameter", "#4DAF4A", "#377EB8")
  
  # Create layout
  l <- layout_with_fr(g)
  
  # Plot the network
  par(mar = c(0,0,2,0))
  plot(g, layout = l,
       vertex.size = 15,
       vertex.label.cex = 0.7,
       vertex.label.color = "black",
       vertex.frame.color = NA,
       edge.curved = 0.3,
       main = paste0("Significant Parameter-Behavior Correlations (p<.05, r>", cor_threshold, ")"))
  
  legend("bottomright", 
         legend = c("Parameter (latent)", "Metric (observed)", 
                   "Positive Correlation", "Negative Correlation"),
         pch = c(16, 15, NA, NA),  # Circle for parameters, square for metrics
         col = c("#4DAF4A", "#377EB8", "#4DAF4A", "#E41A1C"),
         lty = c(NA, NA, 1, 1),
         pt.cex = 2,
         cex = 0.7,
         bty = "n")
  
  # Add a table with correlation values and CIs
  significant_cors %>%
    mutate(
      # Calculate Fisher's z transformation for CI
      z = 0.5 * log((1 + Correlation) / (1 - Correlation)),
      se_z = 1 / sqrt(n_subjects - 3),
      ci_lower_z = z - 1.96 * se_z,
      ci_upper_z = z + 1.96 * se_z,
      # Transform back to correlation scale
      ci_lower = (exp(2 * ci_lower_z) - 1) / (exp(2 * ci_lower_z) + 1),
      ci_upper = (exp(2 * ci_upper_z) - 1) / (exp(2 * ci_upper_z) + 1)
    ) %>%
    select(Parameter, Statistic, Correlation, ci_lower, ci_upper, p_value) %>%
    mutate(
      Parameter = format_names(Parameter),
      Statistic = format_names(Statistic),
      CI = sprintf("[%.2f, %.2f]", ci_lower, ci_upper)
    ) %>%
    select(Parameter, Statistic, Correlation, CI, p_value) %>%
    arrange(desc(abs(Correlation))) %>%
    kable(caption = "Significant Correlations with 95% CIs", digits = c(NA, NA, 2, NA, 3)) %>%
    kable_styling(bootstrap_options = c("striped", "hover", "condensed"),
                 full_width = FALSE) %>%
    print()
} else {
  message("Insufficient significant correlations for network visualization.")
}
```

